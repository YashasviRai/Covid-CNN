# -*- coding: utf-8 -*-
"""covidCNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RNnudiLyNs1fDzkutkzCrrkh7XYMENcE
"""

import numpy as np
import pandas as pd
import matplotlib as mpl
import matplotlib.pyplot as plt
import os
import tensorflow as tf
from tensorflow import keras

# # Creating Train / Val / Test folders (One time use)
import shutil
import random
#root_dir = r'/content/drive/MyDrive/CNN-covid/COVID-19_Radiography_Dataset/'
root_dir = r'C:/Covid/COVID-19_Radiography_Dataset/'
# data root path
classes_dir = ['COVID', 'Normal'] #total labels

val_ratio = 0.15
test_ratio = 0.05

for cls in classes_dir:
#     if os.path.exists(root_dir + 'train/' + cls)=='false':
#       os.makedirs(root_dir + 'train/' + cls)
#     if os.path.exists(root_dir + 'val/' + cls)=='false':
#       os.makedirs(root_dir + 'val/' + cls)
#     if os.path.exists(root_dir + 'test/' + cls)=='false':
#       os.makedirs(root_dir + 'test/' + cls)
    os.makedirs(root_dir + 'train/' + cls)
    os.makedirs(root_dir + 'val/' + cls)
    os.makedirs(root_dir + 'test/' + cls)


    # Creating partitions of the data after shuffeling
    src = root_dir + cls  # Folder to copy images from

    allFileNames = os.listdir(src)
    np.random.shuffle(allFileNames)
    train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),
                                                              [int(len(allFileNames)* (1 - (val_ratio + test_ratio))), 
                                                              int(len(allFileNames)* (1 - test_ratio))])


    train_FileNames = [src+'/'+ name for name in train_FileNames.tolist()]
    val_FileNames = [src+'/' + name for name in val_FileNames.tolist()]
    test_FileNames = [src+'/' + name for name in test_FileNames.tolist()]

    print('Total images: ', len(allFileNames))
    print('Training: ', len(train_FileNames))
    print('Validation: ', len(val_FileNames))
    print('Testing: ', len(test_FileNames))

    # Copy-pasting images
    for name in train_FileNames:
        shutil.copy(name, root_dir +'train/' + cls)

    for name in val_FileNames:
        shutil.copy(name, root_dir +'val/' + cls)

    for name in test_FileNames:
        shutil.copy(name, root_dir +'test/' + cls)

# train_dir = r"/content/drive/MyDrive/CNN-covid/COVID-19_Radiography_Dataset/train"
# val_dir = r"/content/drive/MyDrive/CNN-covid/COVID-19_Radiography_Dataset/val"
# test_dir = r"/content/drive/MyDrive/CNN-covid/COVID-19_Radiography_Dataset/test"
train_dir = r"C:/Covid/COVID-19_Radiography_Dataset/train"
test_dir = r"C:/Covid/COVID-19_Radiography_Dataset/test"
val_dir = r"C:/Covid/COVID-19_Radiography_Dataset/val"
realTime_test_dir = r"C:/Covid/COVID-19_Radiography_Dataset/test2"

from tensorflow.keras.preprocessing.image import ImageDataGenerator

train_datagen = ImageDataGenerator(rescale=1./255)
val_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)
train_generator = train_datagen.flow_from_directory(
    train_dir, 
    target_size=(299, 299),
    batch_size=32,
    class_mode='binary',
    color_mode='grayscale'
)
validation_generator = val_datagen.flow_from_directory(
    val_dir, 
    target_size=(299, 299),
    batch_size=32,
    class_mode='binary',
    color_mode='grayscale'
)
test_generator = test_datagen.flow_from_directory(
    realTime_test_dir,
    target_size=(299, 299),
    batch_size = 1,
    class_mode='binary',
    color_mode='grayscale'
)

import tensorflow
import keras
from tensorflow.keras import layers
from tensorflow.keras import models

def generate_model(struct):
  Conv_layers = struct[0]
  Filters = [struct[1],struct[3],struct[5],struct[7]]
  Poolsize_maxpool = [struct[2],struct[4],struct[6],struct[8]]
  Drpout_per = struct[9]
  Dense_layers = struct[10]
  Dense_neurons = [struct[11],struct[12],struct[13],struct[14],struct[15]]
  Optimizer = struct[16]
  mymodel = models.Sequential()

  for i in range(Conv_layers):
    mymodel.add(layers.Conv2D(Filters[i],kernel_size=(Poolsize_maxpool[i],Poolsize_maxpool[i]),activation= 'relu'))
    mymodel.add(layers.MaxPool2D(pool_size=(Poolsize_maxpool[i],Poolsize_maxpool[i])))
    mymodel.add(layers.Dropout(Drpout_per/100))
  mymodel.add(layers.Flatten())

  for i in range(Dense_layers):
    mymodel.add(layers.Dense(Dense_neurons[i],activation ='relu'))

  mymodel.add(layers.Dense(1,activation='sigmoid'))

  mymodel.compile(
    loss='binary_crossentropy',
    optimizer=Optimize_list[Optimizer],
    metrics=['accuracy'])
  
  checkpoint_cb = keras.callbacks.ModelCheckpoint("Best_Model.h5", save_best_only = True)
  early_stopping_cb = keras.callbacks.EarlyStopping(patience=3, restore_best_weights = True)

  model_info = mymodel.fit_generator(train_generator, 
                                     epochs = 1, 
                                     validation_data=validation_generator, 
                                     callbacks=[checkpoint_cb, early_stopping_cb])
  myloss = model_info.history['loss']
  myaccuracy = model_info.history['accuracy']
  return (myloss, myaccuracy, mymodel, model_info)

#Setting the bounds for all the hyperparameters to be supplied to the jSO algorithm to find the optimal hyperparameter 
HP_Bounds = dict()
HP_Bounds['Conv_and_maxPooling_Layers'] = [3,4]
HP_Bounds['Filters_for_conv1'] = [8,64]
HP_Bounds['PoolSize_maxPool1'] = [3,7]
HP_Bounds['Filters_for_conv2'] = [32,128]
HP_Bounds['PoolSize_maxPool2'] = [3,7]
HP_Bounds['Filters_for_conv3'] = [86,256]
HP_Bounds['PoolSize_maxPool3'] = [3,5]
HP_Bounds['Filters_for_conv4'] = [86,256]
HP_Bounds['PoolSize_maxPool4'] = [3,5]
HP_Bounds['Dropout_Perc'] = [30,50]
HP_Bounds['Dense_Layers'] = [1,5]
HP_Bounds['Dense_neurons_1'] = [256,512]
HP_Bounds['Dense_neurons_2'] = [128,256]
HP_Bounds['Dense_neurons_3'] = [64,128]
HP_Bounds['Dense_neurons_4'] = [32,64]
HP_Bounds['Dense_neurons_5'] = [16,32]
HP_Bounds['Optimizer'] = [0,2]

No_of_Hyperparameters = len(HP_Bounds)

import random
Population_size = 1
fes_max = Population_size * No_of_Hyperparameters
HP_Bound_list= [[3,4],[8,64], [3,7],[32,128],[3,7],[86,256],[3,5],[86,256],[3,5],[30,50],[1,5],[256,512],[128,256],[64,128],[32,64],[16,32],[0,2]]
Optimize_list = ['SGD','RMSprop','Adam']
Population_list=[]
for j in range(Population_size):
  Structure =[]
  for i in range(len(HP_Bound_list)):
    x = random.randint(HP_Bound_list[i][0],HP_Bound_list[i][1])
    Structure.append(x)
  
  Population_list.append(Structure)
fes = Population_size

# training 1st model
var = generate_model(Population_list[0])
print(var[0])
print(var[1])
model_history = var[3]
bestModel = var[2]
accuracy = var[1]
loss = var[0]

# for struct in Population_list:
#  var = generate_model(struct)
#  print(var[0])
#  print(var[1])
#  if var[1]>accuracy:
#    model_history = var[3]
#    bestModel = var[2]
#    accuracy = var[1]
#    loss = var[0]

# summarize history for accuracy
plt.plot(model_history.history['accuracy'])
plt.plot(model_history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(model_history.history['loss'])
plt.plot(model_history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

result = bestModel.predict(test_generator)
print(result)
class_ = (result > 0.5).astype("int32")
print(class_)

output = bestModel.evaluate(test_generator)

x, y = test_generator.next()
print(x)
label = bestModel.predict(x)
output_label = (label > 0.5).astype("int32")
im = x[0]
verdict = "COVID"
if output_label==1:
  verdict = "NORMAL"
plt.imshow(im, cmap="gray")
print(verdict)

bestModel.save("Covid_CNN.h5",)

reloadModel = keras.load_model('Covid_CNN.h5')

